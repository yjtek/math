{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let X and Y be random variables. What is the distribution of their sum T?\n",
    "    - i.e. What is the distribution of T = X+Y?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In principle, we already know how to do this\n",
    "    - If we know what is $f(X)$ and $f(Y)$, then we can derive the joint distribution $f(X=x, Y=y)$\n",
    "    - To find the distribution of $T$, we want to know $f_{T}(t) = P(T=t) = P(X+Y = t)$\n",
    "    - This is simply taking the sum of the joint distribution of X and Y across all values of x+y\n",
    "        - i.e. $P(T=t) = \\sum_{x,y: x+y = t} f(X=x, Y=y)$\n",
    "\n",
    "- Let's reuse the earlier example of roulette wins between Xavier and Yolanda from lesson 18. To avoid repetition, we assume we already have the table of joint probabilities\n",
    "\n",
    "$$\\begin{array}{rr|cccc}\n",
    " & 5 & 0 & 0 & 0 & 0.0238 \\\\\n",
    " & 4 & \\textcolor{blue}{0} & 0 & 0.0795 & 0.0530 \\\\\n",
    "y & 3 & 0 & \\textcolor{blue}{0.0883} & 0.1766 & 0.0294 \\\\\n",
    " & 2 & 0.0327 & 0.1963 & \\textcolor{blue}{0.0981} & 0 \\\\\n",
    " & 1 & 0.0726 & 0.1090 & 0 & \\textcolor{blue}{0} \\\\\n",
    " & 0 & 0.0404 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "& & 0 & 1 & 2 & 3 \\\\\n",
    "& & & & x & &\n",
    "\\end{array}$$\n",
    "\n",
    "- Suppose we are interested in $t=4$. This will simply be the summation of the probabilities in blue above!\n",
    "\n",
    "- For a fixed value of $t$, $x$ determines $y$ through the relation $y = t-x$. So we can rewrite the earlier relation:\n",
    "    - $f_T(t) = P(T=t) = \\sum_{x,y: x+y = t} f(X=x, Y=y) = \\sum_{x} f(x, t-x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theorem 21.1 (Sum of Independent Random Variables): Let X and Y be independent random variables. Then, the PMF of T = X + Y is the convolution of the pmfs of X and Y:\n",
    "    - $f_T = f_X * f_Y$\n",
    "    - The convolution operator `*` is defined as:\n",
    "        - $f_T(t) = \\sum_x f_X(x) \\cdot f_Y(t-x)$\n",
    "    - This follows from the above relation that $f_T(t) = \\sum_xf(x,t-x)$\n",
    "        - Since we have assumed independence, $f_T(t) = \\sum_xf(x,t-x) = \\sum_xf(x) \\cdot f(t-x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theorem 21.2 (Sum of independent binomials): Let X and Y be independent $\\text{Binomial}(n,p)$ and $\\text{Binomial}(m,p)$ random variables, respectively. Then T = X+Y follows a $\\text{Binomial}(n+m,p)$ distribution\n",
    "    - Since X and Y are independent, we use the relationship established above\n",
    "    - $\\begin{align}\n",
    "        f_T(t) &= \\sum_x^t f_X(x) \\cdot f_Y(t-x) \\\\\n",
    "        &= \\sum_x^t \\binom{n}{x} p^x (1-p)^{n-x} \\cdot \\binom{m}{t-x} p^{t-x} (1-p)^{m-t+x} \\\\\n",
    "        &= \\sum_x^t \\binom{n}{x} \\binom{m}{t-x} p^t (1-p)^{n+m-t} \\\\\n",
    "        &= \\binom{n+m}{t} p^t (1-p)^{n+m-t} & \\text{making use of Vandermonde's identity} \\sum_x^t \\binom{n}{x} \\binom{m}{t-x} = \\binom{n+m}{t}\n",
    "    \\end{align}$\n",
    "\n",
    "    - Proving Vandermonde's identity rigorously is beyond the purpose of this set of notes, but we can see it geometrically:\n",
    "        - Previously, we found that $\\frac{\\binom{n}{x} \\binom{m}{t-x}}{\\binom{n+m}{t}}$ is the PMF of the hypergeometric distribution\n",
    "        - As such, $\\sum_{x=0}^{t} \\frac{\\binom{n}{x} \\binom{m}{t-x}}{\\binom{n+m}{t}} = 1$, because this represents all possible values of $x$!\n",
    "        - Vandermonde's identity follows by the multiplication of $\\binom{n+m}{t}$ on both sides of the equation\n",
    "\n",
    "- The conclusion here is that the sum of 2 independent binomial variables must also be binomial!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intuitively, we can see why the sum of independent binomials must also be binomial\n",
    "    - Summing X and Y where both are independent binomial with the same $p$ is akin to drawing from the same box $m+n$ times with replacement\n",
    "    - This is quite simply the same binomial process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
