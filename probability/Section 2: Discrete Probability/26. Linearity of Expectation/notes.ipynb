{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theorem 26.1 (Adding or Multiplying a Constant): Let X be a random variable and a, b be constants. Then:\n",
    "    - E[aX] = a E[X]\n",
    "    - E[X + b] = E[X] + b\n",
    "\n",
    "    - Proof using LOTUS:\n",
    "        - $\\begin{align} \n",
    "            E[aX] &= \\sum_x a \\cdot x \\cdot f(x) & \\text{by LOTUS}\\\\ \n",
    "            &= a \\sum_x x \\cdot f(x) \\\\\n",
    "            &= a E[X] \\\\\n",
    "            \\end{align}$\n",
    "\n",
    "        - $\\begin{align} \n",
    "            E[X + b] &= \\sum_x (x+b) \\cdot f(x) & \\text{by LOTUS}\\\\ \n",
    "            &= \\sum_x x \\cdot f(x) + \\sum_x b \\cdot f(x) \\\\\n",
    "            &= E[X] + b \\sum_x f(x) & \\text{sum of f(x) is 1 by definition}\\\\\n",
    "            &= E[X] + b \n",
    "            \\end{align}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem 26.2 (Linearity of Expectation): Let X and Y be random variables. Then, **no matter what their joint distribution is**, $E[X+Y] = E[X] + E[Y]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's prove this\n",
    "$$\\begin{align}\n",
    "    E[X+Y] &= \\sum_x \\sum_y (x+y) f(x,y) & \\text{by LOTUS} \\\\\n",
    "    &= \\sum_x \\sum_y x \\cdot f(x,y) + \\sum_x \\sum_y y \\cdot f(x,y) \\\\\n",
    "    &= \\sum_x x \\sum_y f(x,y) + \\sum_y y \\sum_x f(x,y) \\\\\n",
    "    &= \\sum_x x f_X(x) + \\sum_y y f_Y(y) \\\\\n",
    "    &= E[X] + E[Y]\n",
    "\\end{align}$$\n",
    "\n",
    "- So no matter what their joint distribution is, the expectation of the sum of random variables is simply the sum of the expectations!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 26.1 (Expected Values in Roulette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In roulette, betting on a single number pays 35-to-1. That is, for each $1 you bet, you win $35 if the ball lands in that pocket.\n",
    "\n",
    "- If we let X represent your net winnings (or losses) on this bet, its p.m.f. is\n",
    "    - x = -1, f(x) = 37/38\n",
    "    - x = 35, f(x) = 1/38\n",
    "\n",
    "- We earlier computed E[X] using the relation $E[X] = \\sum_x x \\cdot f(x)$. \n",
    "- There is a roundabout way to do this that uses the linearity of expectations\n",
    "    - Let W be an indicator variable for whether you win the bet (i.e. w = 0 or 1)\n",
    "        - W is 1 with probability $\\frac{1}{38}$, and 0 with probability $\\frac{37}{38}$\n",
    "        - So $E[W] = \\frac{1}{38}$\n",
    "    - Then $E[X] = E[36W - 1] = 36E[W] - 1 = -\\frac{1}{19}$ \n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 26.2 (Xavier and Yolanda Revisited)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xavier and Yolanda head to the roulette table at a casino. They both place bets on red on 3 spins of the roulette wheel before Xavier has to leave. After Xavier leaves, Yolanda places bets on red on 2 more spins of the wheel. Let X be the number of bets that Xavier wins and Y be the number that Yolanda wins.\n",
    "\n",
    "- In Lesson 25, we calculated E[Y-X], the expected number of additional times that Yolanda wins, by applying 2D LOTUS to the joint p.m.f. of X and Y. The calculation was tedious.\n",
    "    - Recall, we had to compute the joint distribtion f(x,y), and take $\\sum_{x,y} (y-x) f(x,y)$\n",
    "\n",
    "- In this lesson, we see how linearity of expectation allows us to avoid tedious calculations. First, by (26.1) and (26.3), we see that:\n",
    "    - $E[Y-X] = E[Y] - E[X]$\n",
    "    - We know that $X \\sim \\text{Binom}(n=3, p = \\frac{18}{38})$, $Y \\sim \\text{Binom}(n=5, p = \\frac{18}{38})$\n",
    "    - Hence, $E[Y] - E[X] = 5*\\frac{18}{38} - 3*\\frac{18}{38} = 2*\\frac{18}{38} \\sim 0.947$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 26.3 (Expected Value of the Binomial and Hypergeometric Distributions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I am VERY uncomfortable with this \"proof\", because it doesn't feel like it is correct to say that a hypergeometric draw X can be written as $X = Y_1 + Y_2 + ...$. Seems off, because expectation of Y_2 cannot be the expectation of Y_1?\n",
    "\n",
    "- In Lesson 22, we showed that the expected values of the binomial and hypergeometric distributions are the same: $np$. \n",
    "\n",
    "- But the proofs we gave were tedious and did not give any insight into why this formula is true. Letâ€™s prove this formula using linearity of expectation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If X is binomial, then break down X into:\n",
    "    - $X = Y_1 + Y_2 + ... Y_n$\n",
    "    - $Y_i$ is the $i$-th draw from the box\n",
    "    - Since each draw is independent, expectation of each draw is the same, which is $p$\n",
    "    - Hence, $E[X] = E[Y_1] + E[Y_2] + ... E[Y_n] = p + p + ... = np$\n",
    "\n",
    "- If X is hypergeometric, the breakdown is exactly the same. Even thought the draws are not independent, the expectation of each $E[Y_1] = p$ because each Y is still a random draw from the box\n",
    "    - Hence $E[X] = E[Y_1] + E[Y_2] + ... E[Y_n] = p + p + ... = np$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 26.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be a $\\text{Binomial}(n, p)$ random variable. What is $E[X(X-1)]$? In Example 24.3, we calculated this expected value using LOTUS. Here is a trick to calculate using linearity of expectations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this scenario, let X represent the number of 1s in the box\n",
    "    - In that case, X(X-1) is just the number of ways to draw two 1s\n",
    "    - e.g. in a box with n=4, with 3 1s, then X = 3 and X(X-1)=6 ways to draw 2 ones, out of 4*3=12 ways to draw 2 items\n",
    "\n",
    "- Let's define another indicator $Y_{ij}, i \\neq j$, that is 1 when both draws are ones\n",
    "    - $\\sum_{i,j} Y_{ij}, i \\neq j$ represents each of the $n(n-1)$ ways to choose 2 ones from the box \n",
    "\n",
    "- $\\begin{align}\n",
    "    X(X-1) &= \\sum_{i,j: i \\neq j} Y_{ij} \\\\\n",
    "    &= \\sum_{i,j: i \\neq j} E[Y_{ij}] \\\\\n",
    "    &= \\sum_{i,j: i \\neq j} p^2 & \\text{because Y is just the probability that both are 1s} \\\\\n",
    "    &= n(n-1)p^2 & \\text{same answer as 24.3} \\\\\n",
    "    \\end{align}$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
