{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory 43.1: Expectation of continuous random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let $g(x,y)$ be some function that transforms X and Y\n",
    "- Let $f(x,y)$ be the joint PMF of X and Y (i.e. $P(X=x and Y=y)$)\n",
    "- From theory 25.1, $E[g(x,y)] = \\sum_x \\sum_y g(x,y) f(x,y)$. This applies when X and Y are **discrete** random variables\n",
    "- **Theory 43.1**: When X and Y are **continuous** random variables, the expectation is the same, just with the summation replaced by an integral\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[g(x,y)] &= \\int_{x=-\\inf}^{\\inf} \\int_{x=-\\inf}^{\\inf} g(x,y) f(x,y) dy dx \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example 43.1**\n",
    "    - Two points are chosen uniformly and independently along a stick of length 1. What is the expected distance between those two points?\n",
    "    - Let X and Y be the two points. We know that $f(X) = \\frac{1}{b-a} = 1 = f(Y)$\n",
    "    - We want to know what is $E[\\left | x-y \\right |]$ \n",
    "    - Using theory 43.1\n",
    "        - $$\\begin{align}\n",
    "            E[\\left | x-y \\right |] &= \\int \\int_{x \\lt y} g(x,y) f(x,y) + \\int \\int_{x \\gt y} g(x,y) f(x,y) & \\text{using } f(X) = f(Y) = 1 \\\\\n",
    "            &= \\int_{y=0}^{1} \\int_{x=0}^{y} y-x \\text{ } dx dy + \\int_{x=0}^{1} \\int_{y=0}^{x} x-y \\text{ } dy dx \\\\\n",
    "            &= \\int_{y=0}^{1} [xy - \\frac{x^2}{2}]^y_0 \\text{ } dy + \\int_{x=0}^{1} [xy - \\frac{y^2}{2}]^x_0 \\text{ } dx \\\\\n",
    "            &= \\int_{y=0}^{1} y^2 - \\frac{y^2}{2} \\text{ } dy + \\int_{x=0}^{1} x^2 - \\frac{x^2}{2} \\text{ } dx \\\\\n",
    "            &= [\\frac{y^3}{3} - \\frac{y^3}{6}]^1_0 + [\\frac{x^3}{3} - \\frac{x^3}{6}]^1_0 \\\\\n",
    "            &= \\frac{1}{6} + \\frac{1}{6} \\\\\n",
    "            &= \\frac{1}{3}\n",
    "            \\end{align}$$\n",
    "\n",
    "<!-- &= \\int_{x=0}^{y} [\\frac{y^2}{2} - xy]^1_0 \\text{ } dx + \\int_{y=0}^{x} [\\frac{x^2}{2} - xy]^1_0 \\text{ } dy \\\\\n",
    "&= \\int_{x=0}^{y} \\frac{1}{2} - x \\text{ } dx + \\int_{y=0}^{x} \\frac{1}{2} - y \\text{ } dy \\\\\n",
    "&= [\\frac{x}{2} - \\frac{x^2}{2}]^y_0 + [\\frac{y}{2} - \\frac{y^2}{2}]^x_0 \\\\ -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory 43.2: Product of Expectation of independent continuous random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Theory 43.2**: Similar to theory 27.2, if X and Y are independent\n",
    "    - $$\n",
    "        E[XY] = E[X] * E[Y] \\\\\n",
    "        E[g(XY)] = E[g(X)] * E[g(Y)] \\\\\n",
    "        $$\n",
    "    - Proof: Let's start from the definition of the expectation\n",
    "        - $$\\begin{align}\n",
    "            E[XY] &= \\int_{x=-\\inf}^{\\inf} \\int_{y=-\\inf}^{\\inf} g(x,y) f(x,y) dy dx \\\\\n",
    "            &= \\int_{x=-\\inf}^{\\inf} \\int_{y=-\\inf}^{\\inf} xy f(x) \\cdot f(y) dy dx \\\\\n",
    "            &= \\int_{x=-\\inf}^{\\inf} x f(x) \\int_{y=-\\inf}^{\\inf} y \\cdot f(y) dy dx \\\\\n",
    "            &= \\int_{x=-\\inf}^{\\inf} x f(x) E[Y] dx \\\\\n",
    "            &= E[Y] \\int_{x=-\\inf}^{\\inf} x f(x) dx \\\\\n",
    "            &= E[Y] E[X] \\\\\n",
    "            \\end{align}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example 43.2**: Suppose a resistor is chosen uniformly at random from a box containing 1 ohm, 2 ohm, and 5 ohm resistor, and connected to live wire carrying a current (in Amperes) is an $\\text{Exponential}(\\lambda=0.5)$ random variable, independent of the resistor. If R is the resistance of the chosen resistor and I is the current flowing through the circuit, then the power dissipated by the resitor is $P = I^2R$. What is the expected power?\n",
    "\n",
    "- There are 2 ways to do this that we will work through\n",
    "    - Approach 1: Use LOTUS, and take $\\iint I^2R \\text{ } f(I,R)$\n",
    "        - Let's first note that R is a discrete, not a continuous variable.\n",
    "        - Next, note that R and I are independent. This is because the $P(R=r | I=i) = P(R=r)$, and $P(I=i | R=r) = P(I=i)$. i.e. Marginal distributions of $I$ and $R$ do not change with each other's values\n",
    "        - Since they are independent, $f(r,i) = f(r) \\cdot f(i)$\n",
    "        - $$\\begin{align}\n",
    "            E(P) &= E(I^2R) \\\\\n",
    "            &= \\sum_{R=r} \\int_{I=0}^{\\inf} i^2 \\cdot r \\text{ } f(r,i) \\\\\n",
    "            &= \\sum_{R=r} r f(r) \\int_{I=0}^{\\inf} i^2 \\text{ } f(i) \\\\\n",
    "            &= \\sum_{R=r} r f(r) \\int_{I=0}^{\\inf} i^2 \\text{ } \\lambda e^{-\\lambda i} & \\text{Integration by parts twice...}\\\\\n",
    "            &= \\sum_{R=r} r f(r) [ i^2 \\cdot -e^{-\\lambda i} - \\int 2i \\cdot -e^{-\\lambda i} ]^{\\inf}_0 & u=i^2, du = 2i, dv = \\lambda e^{-\\lambda i}, v = -e^{-\\lambda i} \\\\\n",
    "            &= \\sum_{R=r} r f(r) [ i^2 \\cdot -e^{-\\lambda i} - 2i \\frac{1}{\\lambda} e^{-\\lambda i} + \\int \\frac{2}{\\lambda} e^{-\\lambda i}]^{\\inf}_0 & u=2i, du = 2, dv = -e^{-\\lambda i}, v = \\frac{1}{\\lambda} e^{-\\lambda i} \\\\\n",
    "            &= \\sum_{R=r} r f(r) [ i^2 \\cdot -e^{-\\lambda i} - 2i \\frac{1}{\\lambda} e^{-\\lambda i} - \\frac{2}{\\lambda^2} e^{-\\lambda i}]^{\\inf}_0 \\\\\n",
    "            &= \\sum_{R=r} r f(r) \\frac{2}{\\lambda^2} \\\\\n",
    "            &= \\frac{2}{\\lambda^2} \\cdot \\frac{1}{3} \\cdot [1 + 2 + 5] & \\text{by uniform distribution } f(r) = \\frac{1}{3} \\\\\n",
    "            &= \\frac{2}{0.5^2} * \\frac{8}{3} \\\\\n",
    "            &= 8 * \\frac{8}{3} \\\\\n",
    "            &= \\frac{64}{3}\n",
    "            \\end{align}$$\n",
    "    - Approach 2: Apply the linearity of expectations\n",
    "        - Approach 1 is time consuming, because you need to do integration by parts twice. There is an easier way.\n",
    "        - Since I and R are independent, $E[I^2R] = E[I^2] \\cdot E[R]$\n",
    "        - Compute $E[I^2]$\n",
    "            - $E[I^2]$ can be computed using LOTUS $\\int_{0}^{\\inf} i^2 \\lambda e^{-\\lambda i}$ as we did in approach 1\n",
    "            - Alternatively, we make use of the relationship that $Var[I] = E[I^2] - E[I]^2$, and so $E[I^2] = Var[I] + E[I]^2$\n",
    "            - From **Example 39.2**, $Var[I] = \\frac{1}{\\lambda^2}$\n",
    "            - From **Example 37.2**, $E[I] = \\frac{1}{\\lambda}$\n",
    "            - Hence, $E[I^2] = \\frac{1}{\\lambda^2} + (\\frac{1}{\\lambda})^2 = \\frac{2}{\\lambda^2} = 8$\n",
    "        - Compute $E[R]$\n",
    "            - $E[R] = \\frac{1}{3} \\cdot (1+2+5) = \\frac{8}{3}$\n",
    "        - $E[I^2R] = E[I^2] \\cdot E[R] = 8 \\cdot \\frac{8}{3} = \\frac{64}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory 43.3: Linearity of Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Where X and Y are continuous random variables, no matter what their joint distribution is\n",
    "$$\n",
    "    E[X+Y] = E[X] + E[Y]\n",
    "$$\n",
    "\n",
    "- Proof: We will prove this by applying 2D Lotus, setting up the integral for $E[X+Y]$\n",
    "    - $$\\begin{align}\n",
    "        E[X+Y] &= \\int_{}^{} \\int_{}^{} x+y \\text{ } f(x,y) dx dy \\\\\n",
    "        &= \\int_{}^{} \\int_{}^{} x \\text{ } f(x,y) dx dy + \\int_{}^{} \\int_{}^{} y \\text{ } f(x,y) dx dy \\\\\n",
    "        &= \\int_{}^{} x \\int_{}^{} f(x,y) dy dx + \\int_{}^{} y \\int_{}^{} f(x,y) dx dy \\\\\n",
    "        &= \\int_{}^{} x f_X(x) dx + \\int_{}^{} y f_Y(y) dy \\\\\n",
    "        &= E[X] + E[Y]\n",
    "        \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example 43.3** In San Luis Obispo, radioactive particles reach a Geiger counter according to a Poisson process at a rate of $\\lambda = 0.8$ particles per second. What is the expected time that the r-th particle hits the detector?\n",
    "    - Let the expected time between the i-th and i+1-th particle be $T_i$\n",
    "        - $T_i \\sim \\text{Exponential}(\\lambda = 0.8)$\n",
    "        - By definition $T_i$ is independent from $T_j$ when $i \\ne j$  \n",
    "    - Let $CT_i$ be the time that the i-th particle hits the counter\n",
    "        - $CT_1 = T_1$\n",
    "        - $CT_2 = T_1 + T_2$\n",
    "        - $CT_r = T_1 + T_2 + ... + T_r$\n",
    "    - Hence\n",
    "        - $$\\begin{align}\n",
    "            E[CT_r] &= E[T_1 + T_2 + ... T_r] \\\\\n",
    "            &= E[T_1] + E[T_2] + ... E[T_r] \\\\\n",
    "            &= \\frac{1}{\\lambda} + \\frac{1}{\\lambda} + ... \\frac{1}{\\lambda} \\\\\n",
    "            &= \\frac{r}{\\lambda}\n",
    "            \\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
