{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Suppose we generate a random length L (in inches) from the p.m.f. below and draw a square with that sidelength. Calculate $E[L]^2$ and $E[L^2]$. Are they the same? Which one represents the expected area of the square we drew?\n",
    "\n",
    "| $\\imath$ | 1 | 2 | 3 |\n",
    "| --- | --- | --- | --- |\n",
    "f($\\imath $) | 0.2 | 0.5 | 0.3 |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $E[L] = (0.2 + 1 + 0.9)/3 = 0.7$\n",
    "- Hence, $E[L]^2 = 0.7^2 = 0.49$\n",
    "- $E[L^2] = (0.2*1 + 0.5*4 + 0.3*9)/3 = 1.63$\n",
    "\n",
    "- Not the same, expected area of square drawn is 1.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let X be a $\\text{Poisson}(\\mu)$ random variable. Calculate $E[X(X-1)]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X = e^{-\\mu} \\frac{\\mu^x}{x!}$\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[X(X-1)] &= \\sum_{x} f_X(x) \\cdot g(x) \\\\\n",
    "    &= \\sum_{x} e^{-\\mu} \\frac{\\mu^x}{x!} \\cdot x(x-1) \\\\\n",
    "    &= \\sum_{x} e^{-\\mu} \\frac{\\mu^{x-2}}{(x-2)!} \\cdot \\mu^2 \\\\\n",
    "    &= \\mu^2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.939\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "mu=5\n",
    "X=scipy.stats.poisson(mu=mu).rvs(size=10_000)\n",
    "x_xminus1 = X * (X-1)\n",
    "\n",
    "print(np.mean(x_xminus1))\n",
    "print(mu**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Let X be a $\\text{Geometric}(p)$ random variable. Let t be a constant. Calculate $M(t) = E[e^{tX}]$ as a function of t. Statisticians call this the moment generating function of X, while engineers may recognize this function as the Laplace transform of the p.m.f. of X."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X = (1-p)^{n-1} p$\n",
    "- Note that $\\sum_{x=0}^{\\inf} q^x = \\frac{q}{1-q}$\n",
    "    - Let $S_n = 1 + q + q^2 + ... q^n$\n",
    "    - Then $(1-q)S_n = (1-q)(1 + q + q^2 + ... + q^n) = 1-q^{n+1}$\n",
    "    - Then $S_n = \\frac{1-q^{n+1}}{1-q}$\n",
    "    - and $S_n = \\frac{1}{1-q}$ as $n \\rightarrow \\inf$\n",
    "    - So $\\sum_{x=0}^{\\inf} q^x = \\frac{1}{1-q}$\n",
    "    - By similar logic $\\sum_{x=1}^{\\inf} q^x = \\frac{q}{1-q}$\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[e^{tX}] &= \\sum_{x=1}^{\\inf} f_X(x) \\cdot g(x) & \\text{x starts from 1 because PMF is undefined for x=0 in geometric distribution} \\\\\n",
    "    &= \\sum_{x=1}^{\\inf} (1-p)^{x-1} p \\cdot e^{tx} \\\\\n",
    "    &= p \\sum_{x=1}^{\\inf} (1-p)^{x-1} \\cdot e^{tx} \\\\\n",
    "    &= \\frac{p}{1-p} \\sum_{x=1}^{\\inf} (e^{t} \\cdot (1-p))^{x} \\\\\n",
    "\n",
    "    &= \\frac{p}{1-p} \\sum_{x=1}^{\\inf} (e^{t} \\cdot (1-p))^{x} \\\\\n",
    "    &= \\frac{p}{1-p} \\cdot \\frac{(e^{t} \\cdot (1-p))}{1 - (e^{t} \\cdot (1-p))} & \\text{Note that this only applies if } 0 < e^{t} \\cdot (1-p) < 1\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.457955691458513\n",
      "2.5267253194541865\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "p=0.3\n",
    "t=0.2\n",
    "\n",
    "q = np.exp(t) * (1-p)\n",
    "\n",
    "assert(q < 1)\n",
    "assert(q > 0)\n",
    "\n",
    "X=scipy.stats.geom(p=p).rvs(size=10_000)\n",
    "e_powerx = np.exp(t*X)\n",
    "\n",
    "print(np.mean(e_powerx))\n",
    "\n",
    "print((p/(1-p)) * (q / (1-q)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Another resolution to the St. Petersburg Paradox is to consider expected utility U rather than expected wealth W. (“Utility” is the term that economists use for “happiness”.) Because of diminishing marginal utility, the first million dollars is worth more than the next million dollars. One way to model diminishing marginal utility is to assume that $U = log(W)$. Show that the expected utility of the St. Petersburg game is finite, even though the expected winnings is infinite.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We established previously that $W = 2^{N-1}$\n",
    "- We also established that $N \\sim \\text{Geom}(0.5)$\n",
    "- So PMF of N is: \n",
    "    - $f_N(n) = 0.5^{n-1} 0.5 = 0.5^n$\n",
    "\n",
    "- Applying LOTUS:\n",
    "    - $\\begin{align}\n",
    "        E[W] &= E[2^{n-1}] \\\\\n",
    "        &= \\sum_{n=1}^{\\inf} 2^{n-1} 0.5^n \\\\\n",
    "        &= \\sum_{n=1}^{inf} \\frac{1}{2}\n",
    "    \\end{align}$\n",
    "\n",
    "- We do the same thing, but applying one more transformation:\n",
    "    - $\\begin{align}\n",
    "        E[U] &= E[log(W)] \\\\\n",
    "        &= E[log(2^{n-1})] \\\\\n",
    "        &= \\sum_{n=1}^{\\inf} log(2^{n-1}) 0.5^n \\\\\n",
    "    \\end{align}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Let X be a $\\text{Hypergeom}(n, N_1, N_0)$ random variable. Calculate E[X(X-1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X = \\frac{\\binom{N_1}{x} \\binom{N_0}{n-x}}{\\binom{N}{n}}$\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[X(X-1)] &= \\sum_x g(x) f_X(x) \\\\\n",
    "    &= \\sum_x x(x-1) \\frac{\\binom{N_1}{x} \\binom{N_0}{n-x}}{\\binom{N}{n}} \\\\\n",
    "    &= \\sum_x x(x-1) \\frac{\\frac{N_1!}{(x)!(N_1 - x)!} \\frac{N_0!}{(n-x)!(N_0 - n + x)!}}{\\frac{N!}{n!(N-n)!}} \\\\\n",
    "    &= \\sum_x \\frac{N_1(N_1-1) \\frac{(N_1 - 2)!}{(x-2)!(N_1 - x)!} \\frac{N_0!}{(n-x)!(N_0 - n + x)!}}{\\frac{N(N-1)(N-2)!}{n(n-1)(n-2)!(N-n)!}} \\\\\n",
    "    &= \\frac{N_1(N_1-1)}{\\frac{N(N-1)}{n(n-1)}} \\sum_x \\frac{\\frac{(N_1 - 2)!}{(x-2)!(N_1 - x)!} \\frac{N_0!}{(n-x)!(N_0 - n + x)!}}{\\frac{(N-2)!}{(n-2)!(N-n)!}} \\\\\n",
    "    &= N_1(N_1-1) \\cdot \\frac{n(n-1)}{N(N-1)} \\\\\n",
    "    &= \\frac{N_1}{N} \\cdot \\frac{N_1-1}{N-1} \\cdot n(n-1) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.7036\n",
      "33.39393939393939\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "n=20\n",
    "n1 = 30\n",
    "n0 = 70\n",
    "N = n1 + n0\n",
    "\n",
    "X=scipy.stats.hypergeom(M=N, n=n1, N=n).rvs(size=10_000)\n",
    "x_xminus1 = X * (X-1)\n",
    "\n",
    "print(np.mean(x_xminus1))\n",
    "print((n1/N) * ((n1-1)/(N-1)) * n * (n-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let X be a $\\text{Poisson}(\\mu)$ random variable for $0 < \\mu < 1$. Calculate E[X!]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X = e^{-\\mu} \\frac{\\mu^x}{x!}$\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[X!] &= \\sum_x g(x) f_X(x) \\\\\n",
    "    &= \\sum_x x! \\cdot e^{-\\mu} \\frac{\\mu^x}{x!} \\\\\n",
    "    &= e^{-\\mu} \\sum_x \\mu^x \\\\\n",
    "    &= e^{-\\mu} \\frac{1}{1 - \\mu} & \\sum_{k=0}^{n} q^k = \\frac{1}{1-q} \\text{ as n tends to infinity}\\\\\n",
    "    &= \\frac{e^{-\\mu}}{1 - \\mu} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "- If step 4 is confusing, see note in question 3. Note that this holds only if $\\mu$ is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0384010440952065"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from math import factorial\n",
    "mu = 0.25 ##Note that this holds only if mu is between 0 and 1\n",
    "\n",
    "assert(0 < mu < 1)\n",
    "\n",
    "X=scipy.stats.poisson(mu=mu).rvs(size=10_000)\n",
    "x_fact = np.array([factorial(x) for x in X])\n",
    "\n",
    "print(np.mean(x_fact))\n",
    "(np.exp(-mu)) / (1-mu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Let X be a $\\text{NegativeBinomial}(r, p)$ random variable. Calculate E[(X+1)X]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X = \\binom{x-1}{r-1} p^{r} (1-p)^{x-r}$\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[(X+1)X] &= \\sum_{x=r}^{\\inf} x(x+1) \\binom{x-1}{r-1} p^{r} (1-p)^{x-r} \\\\\n",
    "    &= \\sum_{x=r}^{\\inf} x(x+1) \\frac{(x-1)!}{(r-1)! (x-r)!} p^{r} (1-p)^{x-r} \\\\\n",
    "    &= \\frac{r(r+1)}{p^2} \\sum_{x=r}^{\\inf} \\frac{(x+1)!}{(r+1)! (x-r)!} p^{r+2} (1-p)^{x-r} \\\\\n",
    "    &= \\frac{r(r+1)}{p^2} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.4164\n",
      "333.33333333333337\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from math import factorial\n",
    "\n",
    "r = n = 5 #number of successes desired\n",
    "p = 0.3 #probability of success\n",
    "\n",
    "'''\n",
    "Note that .rvs() method returns the count of NEGATIVE DRAWS, not total draws\n",
    "You can see this also by the fact that scipy.stats.nbinom.rvs(n=r, p=p, k=0) is not zero, \n",
    "because it is telling you the probability of having 0 NEGATIVE DRAWS before 5 successes, not 0 total draws\n",
    "'''\n",
    "X=scipy.stats.nbinom.rvs(n=r, p=p, size=10_000) + r  \n",
    "x_xplus1 = X * (X+1)\n",
    "\n",
    "print(np.mean(x_xplus1))\n",
    "print((r*(r+1)) / p**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
