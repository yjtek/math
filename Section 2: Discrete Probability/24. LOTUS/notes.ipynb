{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we discussed the St Petersburg Paradox and computed the expected amount $E[W]$. \n",
    "- We computed the PMF of W \n",
    "    - e.g. P(W=1) = 0.5^1, P(W=2) = 0.5^2, ...\n",
    "- We then multiplied each value of the PMF against the reward we get, and took the sum to get the expected value of the bet\n",
    "\n",
    "This was feasible because the PMF of W was easy to find. But in many cases, the PMF of W is not actually clear. Is there a way to find the expectation without computing the PMF of W?\n",
    "- Here, we know that $W$ is related to the number of tosses $N$\n",
    "    - $W = 2^{N - 1}$\n",
    "- Hence, to find $E[W]$, we can simply find $E[2^{N-1}]$\n",
    "\n",
    "More generally, if the PMF of some process $W$ is too difficult to compute, we can derive the expectation of the process $E[W]$ from any known alternative PMF $N$, so long as we can state a relation between $W$ and $N$!\n",
    "\n",
    "This is known as the Law of the Unconscious Statistician, or LOTUS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theorem 24.1 (LOTUS): Let X be a random variable with PMF $f_X(x)$. Let $Y = g(X)$ for some function g. By LOTUS: \n",
    "    - $\\begin{align}\n",
    "        E[Y] &= \\sum_{y=0}^{\\inf} y \\cdot f_Y(y) \\\\\n",
    "        &= \\sum_{x=0}^{\\inf} g(x) \\cdot f_X(x) \\\\\n",
    "        &= E[g(x)]\n",
    "    \\end{align}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 24.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's say we toss a fair coin twice. Let X be the number of heads. The PMF of X is:\n",
    "    - x: 0, f_X(x) = 0.25\n",
    "    - x: 1, f_X(x) = 0.50\n",
    "    - x: 2, f_X(x) = 0.25\n",
    "\n",
    "- Using the value of X from the two tosses, we sketch a circle with radius x. Let A be the area of the circle we sketch\n",
    "    - So, A can be $\\pi(0^2)$, $\\pi(1^2)$, $\\pi(2^2)$\n",
    "\n",
    "- What is E[A]? By our earlier conclusion:\n",
    "    - $\\begin{align}\n",
    "        E[A] &= E[\\pi(X^2)] \\\\\n",
    "         &= \\pi E[X^2] \\\\\n",
    "         &= \\pi (0.25*0 + 0.5*1 + 0.25*4) \\\\\n",
    "         &= 1.5\\pi \n",
    "        \\end{align}$\n",
    "\n",
    "- Note that $E[X^2] \\neq E[X]^2$! \n",
    "    - $\\begin{align}\n",
    "        \\pi E[X]^2 &= \\pi(0*0.25 + 1*0.5 + 2*0.25) \\\\\n",
    "        &= \\pi\n",
    "        \\end{align}$\n",
    "\n",
    "- More generally, $E[g(x)] != g(E[X])$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 24.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's study the St Petersburg paradox once more\n",
    "- Let N be the number of flips in each game\n",
    "    - Since we flip until we get a heads, N follows a Geometric distribution\n",
    "    - The PMF of N must be\n",
    "        - $f_N(n) = (1-0.5)^{n-1} 0.5 = 0.5^n$\n",
    "\n",
    "- Since we know that $W = 2^{N-1}$, by LOTUS we conclude that $E[W] = E[2^{N-1}]$\n",
    "\n",
    "$\\begin{align}\n",
    "    E[W] &= E[2^{N-1}] \\\\\n",
    "    &= \\sum_{N=1}^{\\inf} 2^{N-1} \\cdot 0.5^N & \\text{using }f_N(n) = 0.5^n \\\\ \n",
    "    &= \\sum_{N=1}^{\\inf} 2^{N-1} \\cdot 2^{-N} \\\\ \n",
    "    &= \\sum_{N=1}^{\\inf} \\frac{1}{2} \\\\ \n",
    "    &= \\inf\n",
    "\\end{align}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 24.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be a $\\text{Binomial}(n,N1,N0)$ random variable. In Example 22.2, we showed that $E[X] = np = n \\frac{N_1}{N}$. Now, we calculate $E[X(X-1)]$ by applying LOTUS to the binomial PMF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let X be a binomial random variable. The PMF of X is:\n",
    "    - $f(x) = \\binom{n}{x} p^{x} (1-p)^{n-x}, x=0,1,...n$\n",
    "\n",
    "- Note that can think of $X(X-1)$ as a transformation $g(X)$\n",
    "\n",
    "$$\\begin{align}\n",
    "    E[X(X-1)] &= \\sum_{x=0}^n x(x-1) \\cdot \\binom{n}{x}p^{x}(1-p)^{n-x} \\\\\n",
    "    &= \\sum_{x=2}^n x(x-1) \\cdot \\binom{n}{x}p^{x}(1-p)^{n-x} & \\text{because summand is 0 for x=0,1} \\\\ \n",
    "    &= \\sum_{x=2}^n x(x-1) \\frac{n!}{x! (n-x)!} \\cdot p^{x}(1-p)^{n-x} \\\\\n",
    "    &= \\sum_{x=2}^n n(n-1)\\frac{(n-2)!}{(x-2)! (n-x)!} \\cdot p^{x}(1-p)^{n-x} \\\\\n",
    "    &= n(n-1) \\sum_{x=2}^n \\binom{n-2}{x-2} \\cdot p^{x}(1-p)^{n-x} \\\\\n",
    "    &= n(n-1) p^2 \\sum_{x=2}^n \\binom{n-2}{x-2} \\cdot p^{x-2}(1-p)^{n-x} \\\\\n",
    "    &= n(n-1) p^2 & \\text{since the sum is a PMF and must sum to 1}\\\\\n",
    "    \n",
    "\\end{align}$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
