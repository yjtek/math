{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70512cb",
   "metadata": {},
   "source": [
    "## Cauchy-Schwarz Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1217a",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7656c",
   "metadata": {},
   "source": [
    "- The Cauchy-Schwarz Inequality states that, for any 2 vectors, $(x,y)$, the absolute value of their dot product is at most the products of their magnitudes\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    |x \\cdot y| \\le \\|x\\| \\|y\\|\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b29069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([1,2,3])\n",
    "y=np.array([2,3,4])\n",
    "\n",
    "dot_product = np.abs(x @ y)\n",
    "product_of_norms = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "dot_product <= product_of_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e6d86",
   "metadata": {},
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34418cdf",
   "metadata": {},
   "source": [
    "- Let's imagine we have the 2 vectors, $x, y \\in \\mathbb{R}^N$, and an arbitrary scala $\\lambda \\in \\mathbb{R}$. Note that these must be **VECTORS**, not matrices\n",
    "\n",
    "- It must be true that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\| x - \\lambda y \\|^2 \\ge 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Why? Because the 2 norm is always non-negative (since it is the squareroot of the sum of squares), and squaring the 2 norm is therefore also always non-negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b269b4ec",
   "metadata": {},
   "source": [
    "- Expanding this, we get\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\| x - \\lambda y \\|^2 &\\ge 0 \\\\\n",
    "    (x - \\lambda y) \\cdot (x - \\lambda y) &\\ge 0 \\\\\n",
    "    (x \\cdot x) - 2 x \\lambda y + \\lambda^2 (y \\cdot y) &\\ge 0 \\\\\n",
    "    \\|x\\|^2 - 2 \\lambda x \\cdot y + \\lambda^2 \\|y\\|^2 &\\ge 0 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Now, note that $\\|x\\|$, $\\|y\\|$, and $x \\cdot y$ are just scalars. So really, this is a quadratic equation with $\\lambda$ as the unknown. Rewriting:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\|y\\|^2 \\lambda^2 - 2 (x \\cdot y) \\lambda + \\|x\\|^2 &\\ge 0 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886cadf",
   "metadata": {},
   "source": [
    "- Now, let's consider what properties the quadratic equation must have for the inequality above to be true\n",
    "    1. For a quadratic to be non-negative across the entire domain of $\\mathbb{R}$, it must be an upwards parabola (i.e. a U shaped curve). \n",
    "    2. The discriminant of the quadratic must be non-positive\n",
    "        - Why? Because if we want the curve to lie strictly above the x-axis, it must be true that there is at most 1 real root\n",
    "\n",
    "- Condition 1 is satisfied by definition, because $|y|^2$ must be non-negative\n",
    "\n",
    "- For Condition 2 to be true, $b^2 - 4ac$ must either be negative (so the square root is invalid), or 0 (so we end up with only 1 real root)\n",
    "\n",
    "- Therefore:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    b^2 - 4ac &\\le 0 \\\\\n",
    "    4 |x \\cdot y |^2 - 4 \\|y\\|^2 \\| x \\|^2 &\\le 0 \\\\\n",
    "    4 |x \\cdot y |^2 &\\le 4 \\|y\\|^2 \\| x \\|^2 \\\\\n",
    "    |x \\cdot y |^2 &\\le \\|y\\|^2 \\| x \\|^2 \\\\\n",
    "    \\therefore |x \\cdot y | &\\le \\|y\\| \\| x \\|\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Note that taking square root in the last step is valid because both sides are strictly positive\n",
    "\n",
    "- This gives us the Cauchy-Schwarz Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51217f7",
   "metadata": {},
   "source": [
    "#### Interesting follow up:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16032c8a",
   "metadata": {},
   "source": [
    "- There is an interesting application of the CSI when combined with the Law of Cosines\n",
    "\n",
    "- Recall that the following is true for any angle $\\theta$, where $\\theta$ is the angle between vectors. (See `4 - Law of Cosines.ipynb` for details)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{|x \\cdot y|}{\\| x \\| \\| y \\|} &= \\cos(\\theta)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- But from Cauchy-Schwarz inequality, we know that $|x \\cdot y| \\le \\|x\\|\\|y\\|$. Therefore\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{|x \\cdot y|}{\\| x \\| \\| y \\|} &= |\\cos(\\theta)| \\\\\n",
    "    &\\le 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Which is a fundamental truth of trigonometry, since $|\\cos{\\theta}| \\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59660e",
   "metadata": {},
   "source": [
    "### Equality Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eb1ec",
   "metadata": {},
   "source": [
    "- Cauchy-Schwarz also applies when $|x \\cdot y | = \\|x\\| \\|y\\|$\n",
    "\n",
    "- This occurs in the equality case where $x$ and $y$ are linearly dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8ca0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(28), np.float64(28.0), np.int64(42), np.float64(42.0))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = x*2\n",
    "z = x*3\n",
    "\n",
    "abs_dot_product = x @ y\n",
    "product_of_norms = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "\n",
    "abs_dot_product2 = x @ z\n",
    "product_of_norms2 = np.linalg.norm(x) * np.linalg.norm(z)\n",
    "\n",
    "abs_dot_product, product_of_norms, abs_dot_product2, product_of_norms2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
